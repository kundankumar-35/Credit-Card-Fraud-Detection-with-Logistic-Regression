# Credit Card Fraud Detection with Logistic Regression


import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load the uploaded CSV file
creditCard = pd.read_csv('creditcard_csv.csv')

# print(creditCard.shape)


print(creditCard.shape)
# display(creditCard.describe())
# display(creditCard.tail())

creditCard.info()

**checking the missing data in each data **

creditCard.isnull().sum()

# Check the number of legal and fraudulent transactions
class_counts = creditCard['Class'].value_counts()
display(class_counts)

# Separate the dataset into legal and fraudulent transactions
legal_transactions = creditCard[creditCard['Class'] == "'0'"]
fraudulent_transactions = creditCard[creditCard['Class'] == "'1'"]

# Display the shapes of the new dataframes to verify the separation
print("Shape of legal transactions dataframe:", legal_transactions.shape)
print("Shape of fraudulent transactions dataframe:", fraudulent_transactions.shape)

legal_transactions.Amount.describe()

fraudulent_transactions.Amount.describe()

comapre the values for both transation


creditCard.groupby('Class').mean()

# Undersample the legal transactions to have the same number of instances as fraudulent transactions
legal_transactions_undersampled = legal_transactions.sample(n=len(fraudulent_transactions), random_state=42)

# Concatenate the undersampled legal transactions with the fraudulent transactions
undersampled_creditCard = pd.concat([legal_transactions_undersampled, fraudulent_transactions], axis=0)

# Display the new class distribution
print("Class distribution after undersampling:")
display(undersampled_creditCard['Class'].value_counts())

undersampled_creditCard.groupby('Class').mean()

# Define features (X) and target (y)
X = undersampled_creditCard.drop('Class', axis=1)
y = undersampled_creditCard['Class']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

# Load the Logistic Regression model
model = LogisticRegression()

# Train the Logistic Regression model
model.fit(X_train, y_train)

from sklearn.preprocessing import StandardScaler

# Scale the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

Now that we have scaled the data, let's retrain the Logistic Regression model on the scaled training data.

# Retrain the model on the scaled data
model.fit(X_train_scaled, y_train)

from sklearn.metrics import accuracy_score

# Make predictions on the scaled test data
X_test_scaled = scaler.transform(X_test) # Ensure X_test is scaled using the same scaler
y_pred = model.predict(X_test_scaled)

# Calculate the accuracy score
accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy Score: {accuracy}")

# Make predictions on the scaled training data
y_train_pred = model.predict(X_train_scaled)

# Calculate the accuracy score for the training data
train_accuracy = accuracy_score(y_train, y_train_pred)

print(f"Accuracy Score on Training Data: {train_accuracy}")
